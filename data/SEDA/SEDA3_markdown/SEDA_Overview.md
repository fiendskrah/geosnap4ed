# Stanford Education Data Archive (SEDA)

[Link](https://exhibits.stanford.edu/data/catalog/db586ns4974)

## Home 
- Author
    - Reardon
    - Sean
    - Kalogrides
    - Demetra 
    - Ho
    - Andrew
    - Shear
    - Ben
    - Shores
    - Kenneth
    - Fahle
    - Erin
    - Jang
    - Heewon
    - Chavez
    - Belen 
- Data Story 
    - Improving Educational Equity
- Description 
    - Topic Sentence
        - Racial, socioeconomic, and gender disparities in academic performance and educational attainment are stubborn features of the U.S. educational system 
            - these disparities are neither inevitable nor immutable 
    - Factors
        - disparities have been produced by- and so may also be reduced by- a welter of social and economic policies, social norms, and patterns of interaction, and the organization of American schooling 
    - SEDA 
        - is an intiative aimed at harnessing data to help us 
         


## Collections

## Data Stories 

## About 


# The Educational Opportunity Project at Stanford University 
[Link](https://edopportunity.org) 

Website Directory 
- Home 
    - description
        - The Educational Opportunity Project at Stanford University has built the first national database of academic performance
            - Use our Explorer to view three measures of educational opportunity in your school or community 
        - We're meausring educational opportunity in every community in America
    - Three measures
        - Average test scores
            - description
                - the educational opportunities available in a community, both in and out of school, are reflected in students' average test scores
                - they are influenced by opportunies to learn at home, in neighborhoods, in child-care, preschool, and after-school programs, from peers and friends, and at school 
            - x axis: low opportunity to high opportunitty
            - y axis: scores 
        - Learning Rates
            - description
                - learning rate measure how much students' scores improve each year while they are in school
                - they are a better indicator of school quality than average test scores, which are influenced by a range of experiences outside of school
            - x axis: low growth school to high growth school
            - y axis: scores 
        - Trends in test scores
            - description
                - tracking average test scores over time shows growth or decline in educational opportunity
                - these trends reflect shifts in school quality as well as changes in family and community characteristics
            - x axis: 2009 to 2016
            - y axis: opportunity 
    - Ready to start exploring?
        - view charts and maps that show how counties, school districts, and individual schools score on these three metrics
        - you can also filter by demographics, explore opportunity gaps, export PDF reports, and more 
- Opportunity Explorer 
- Discoveries
    - see what we're learning: read out featured articles, illustrated with dynamic charts & graphics 
    - Sean F. Readon
        - Affluent School Are Note Always the Best Schools
        - What Explains White-Black Differences in Average Test Scores? 
- About 
    - Background
        - The Stanford Education Data Archive (SEDA) is 
            - an intiative aimed at harnessing data to help scholars, policymakers, educators, and parents learn how to improve educational opportunity for all children 
    - Description 
        - part 1
            - racial, socioeconomic, and gender disparities in academic performance and educational attainment are stubborn features of the U.S. educational system
            - these disparities are neither inevitable nor immutable, however 
            - they have been produced by - and so may also be reduced by - a welter of social and economic policies, social norms, and patterns of interaction, and the organization of American schooling
        - part 2 
            - SEDA includes a range of detailed data on educational conditions, contexts, and outcomes in school districts and counties across the United States 
            - it includes measures of academic achievement and achievement gaps for school districts and counties, as well as district-level measures of racial and socioeconomic composition, racial and socioeconomic segregation patterns, and other features of the schooling system
            - the data are pubicly available here, so that anyone can obtain detailed information about American schools, communities, and student success 
    - Goal
        - We hope that researchers will use the data to generate evidence about what policies and contexrs are most effective at increasing educational opportunity, and that such evidence will inform educational policy and practices 
- FAQ 
    - Glossary 
        - Average test score
            - description
                - indicates how well the average student in a school, district, or county performs on standardized tests
                - many factors - both early in life and when children are in school - affect test performance 
                - as a result, the average test scores in a school, district, or county, reflect the total set of educational opportunities children have had from birth through middle school, including opportunities at home, in child-care and preschool programs, and among peers
                - Average test scores therefore reflect a mix of school quality and out-of-school educational opportunities 
        - Learning rate
            - indicates approximately how much students learn in each grade in a school, district, or county
            - because most educational opportunities in grades 3-8 are provied by schools, the average learning rate largely reflects school quality 
        - Trend in test scores
            - indicates how rapidly average test scores within a school, district, or county have changed over time
            - it reflects changes over time in the total set of educational opportunities (in and out of schools) available for children
            - for example, average scores might improve over time because the schools are improving and or/ because more high-income families have moved into the community 
        - Educational opportunity
            - a child's educational opportunities include all experiences that contribute to learning the skills assessed on standardized tests
            - these include opportunities both in early childhood and during the schools years, and experiences in homes, in neighborhoods, in child-care and preschool programs, with peers, and in schools 
        - SES
            - socioeconomic status
            - How is socioeconomic status measured?
                - describes the variables used to measure the average socioeconomic composition of families in each community, both overall and for different groups within a community 
        - Grade Level
            - as described in the Methods section, the test-score in SEDA are adjusted so that a "4" represents the average test scores of 4th graders nationally, a "5" represents the average test scores of 5th graders nationally, and so, on. One "grade level" thus corresponds to the average per-grade increase in test scores for students nationally 
        - Geographic school district
            - each traditional public-school district in the U.S. is defined by a geographic catchmeent area; the schools that fall within this boundary make up the geographic school district
        - Gap in School Poverty
            - is a measure of school segregation
            - we use the proportion of students defined as "economically disadvantaged" in a school as a measure of school poverty
            - the black-white gap in school poverty, for example, measures the difference between the poverty rate of the average black student's school and the poverty rate of the average white students' school
            - when there is no segregation- when white and black students attend the same schools, or when white and black students' schools have equal poverty rates 
                - the black-white poverty gap is zero 
            - a positive black-white school poverty gap means that black students' schools have higher poverty rates than white students' schools, on average
            - a negative black-white school poverty gap means that white students' schools have higher poverty rates than black students' schools, on average 
        - Gap in Percent Minority Students in Schools
            - the gap in percent minority students in schools is a measure of school segregration
            - percent minority students in schools is measured as the proportion of minority students (black students plus Hispanic students) in a student's school 
            - the black-white gap in percent minority students in schools then measures the difference between the proportion of minority students in the average black student's school and
                -  the proportion of minority students in the average white student's school
            - when there is no segregation 
                - when white and black students attend the same schools, or when white and black students' schools have equal proportions of minority students
                    - the black-white gap in percent minority students is 0
            - a positive black-white gap in percent minority students means that black students' schools have higher shares of minority students than white students' schools, on average
            _ a negative black-white gap in percent minority students means that white students' school have higher shares of minority students than black students' schools, on average 
        - Cohort
            - a group of students beginning school together in the same year
            - For example, the 2006 cohort refers to students who were first enrolled in kindergarten in the fall of 2005 (and finished in the spring of 2006)
            - while some of these students may repreat or skip a grade during school, the majority will progress through school together in the same grades each year 
        - Average
            - the average, or "mean," is used to represent the typical or central value in a set of numbers 
        - Standard deviation
            - indicates how spread out a set of numbers is relative to the average
            - standard devidation represents the typical amount by which any single number differs from the average 
        - Standardized 
            - when numbers in a set have been "standardized," they have been transformed so that the average value is exactly 0
                - the standard deviation is exactly 1 
            - after being standardized, positive numbers present values that are above the average and negative numbers represent values that are below the average
        - Achievement test
            - a standardized test used to measure the knkowledge and skills a student has attained in a particular subject, after instruction 
        - Test score
            - the numeric score earned on an achievement test 
        - Achievement level (or proficiency level or proficiency category)
            - a description of what test-takers earning scores in a particular range are expected to known 
                - and/pr are able to do
            - each achievement test usually has three to five achievements levels defined
            - achievement levels often have labels describing performance relative to grade-level expectations, such as "basic," "proficient," or "advanced" 
        - Cut score/threshold
            - the test scores, or boundaries, taht separate a set of achievement levels
        - Proficiency data
            - a data set containing information about how many students in each group (where graoups could be schools, district,states, etc) earned scores in each achievement level
        - Proficiency rate
            - the percentage of students whose test scores were at or above the "proficient" achievement level in a given grade 
        - Linking
            - a set of statistical methods used to facilitate comparisons across different tests
        - NAEP
            - the national assessment of educational progress
        - HETOP
            - the heteroskedastic ordered probhit model 
                - is a statistical technique that can be used to analyze proficiency data 
        - ECD
            - economically disadvantaged students 
        - FRPL 
            - free or reduced-price-lunch-eligible students 
    - Understanding the data
        - Where do the test score data come from? What years, grades, and subjects are used?
            - The data are based on the standardized accountability tests in math and English Language Arts (ELA) administered annually by each state to all public-school students in grades 3–8 from 2008–09 through 2015–16. In these years, 3rd through 8th graders in U.S. public schools took roughly 350 million standardized math and ELA tests. Their scores—provided to us in aggregated form by the U.S. Department of Education—are the basis of the data reported here.
            - We combine information on the test scores in each school, school district, or county with information from the National Assessment of Educational Progress 
                - (NAEP; see https://nces.ed.gov/nationsreportcard/about/) to compare scores from state tests on a common national scale (see see the Methods page)
            - We never see nor use individual test scores in this process. The raw data we receive includes only counts of students scoring at different test-score levels, not individual test scores. There is no individual or individually-identifiable information included in the raw or public data.
        - What are educational opportunities? 
            - Educational opportunities include all experiences that help a child learn the skills assessed on standardized tests. These include opportunities both in early childhood and during the schooling years, and experiences in homes, in neighborhoods, in child-care and preschool programs, with peers, and in schools.
        - What do average test scores tell us?
            - To understand the role of educational opportunities in shaping average test-score patterns, it is necessary to distinguish between individual scores and average scores in a given school, district, or county.
            - Differences in two students’ individual test scores at a given age reflect both differences in their individual characteristics and abilities and differences in the educational opportunities they have had. However, because the average innate abilities of students born in one community do not differ from those born in another place, any difference in average test scores must reflect differences in the educational opportunities available in the two communities.
        - Why are there three different summaries of test scores (average scores, learning rates, and trends in scores) in each place? What can we learn from each of these?
            - The three scores tell different stories.
                - **Average test score**:
                    - The average test score indicates how well the average student in a school, district, or county performs on standardized tests. Importantly, many factors—both early in life and when children are in school—affect test performance. As a result, the average test scores in a school, district, or county reflect the total set of educational opportunities children have had from birth through middle school, including opportunities at home, in child-care and preschool programs, and among peers. Average test scores therefore reflect a mix of school quality and out-of-school educational opportunities.
                - **Learning rate**: 
                    - The learning rate indicates approximately how much students learn in each grade in a school, district, or county. Because most educational opportunities in grades 3–8 are provided by schools, the average learning rate largely reflects school quality.
                - **Trend in test scores**:
                    - The trend in scores indicates how rapidly average test scores within a school, district, or county have changed over time. It reflects changes over time in the total set of educational opportunities (in and out of schools) available to children. For example, average scores might improve over time because the schools are improving and/or because more high-income families have moved into the community.
        - How are average test scores, average learning rates, and average test score trends computed?
            - Once the test scores are placed on a common scale across states, grades, and years, we have measures of the average test scores in up to 96 grade-year-subject cells for each school, district, or county. The scores are adjusted so that a value of 3 corresponds to the average achievement of 3rd graders nationally, a value of 4 corresponds to the average achievement of 4th graders nationally, and so on.
            - **Average test scores**:
                - To compute the average test score, we compare students in each grade, year, and subject with the national average and then combine them.
            - **Learning rate**:
                - To compute the average learning rate, we compare students’ average scores in one grade and year to those in the next grade and year. For example, we compare the average math score in grade 3 in 2009 to the average math score in grade 4 in 2010. The difference is an estimate of the learning rate in a specific grade and subject for a specific cohort of students. We then compute the same change for each cohort, grade, and subject for which we have data. The average of these grade-to-grade changes (across cohorts, grades, and subjects) is the learning rate. A learning rate of 1.0 indicates that average student’s scores improve by one grade level each grade.
            - **Trend in test scores**:
                - To compute trends in test scores, we compare students’ average scores in one grade and year to those in the same grade in the next year. For example, we compare the average math score in grade 3 in 2009 to the average math score in grade 3 in 2010. The difference between these two values indicates how much math scores improved in one year in grade 3. We then compute the same change for each year, grade, and subject. The average of these annual changes (across years, grades, and subjects) is the trend in test scores.
        - What schools are included in the data?
            - All public schools—both traditional and charter schools—that serve students in any grade from 3 through 8 are included in the data.
            - The data include public schools that enroll students in grades 3–8. Schools that enroll both high school students and students in some of grades 3–8 are included, but the reported test-score measures are based only on the scores of students in grades 3–8.
                - For example, schools serving grades 7–12 are included in the data, but the test scores we use are only those from students in grades 7–8.
            - In the district data, charter schools are considered part of the traditional public-school district chartering them. Charter schools not chartered by a district are considered to be part of the district in which they are physically located. A data file indicating which schools are charter schools and what districts they are associated with in the data is available from the Get the Data page.
            - We do not use or report test scores from virtual schools.
            - In most states, students in private schools do not take the annual state accountability tests that we use to build SEDA. Therefore, no test scores from private schools are included in the raw or public data.
        - How accurate are the data? What should I do if I find an error in the data? 
            - In most states, students in private schools do not take the annual state accountability tests that we use to build SEDA. Therefore, no test scores from private schools are included in the raw or public data.
        - Are these data available for researchers or others to use?
            - Yes. These data are available for researchers to use under the terms of our data use agreement. You can download the data files and full technical documentation on our Get the Data page.
        - Are the test-score measures (average scores, learning rates, and trends) adjusted to take into account differences in student demographic characteristics or any other students or school variables? 
            - No. The measures of average test scores, average learning rates, and average test-score trends are based solely on test-score data.
            - On the website, we note schools that serve exceptional student populations (large proportions of students with disabilities, students enrolled in gifted/talented programs, or students with limited English proficiency). These students’ characteristics should be taken into consideration when interpreting the test-score data and comparing performance to that of public schools serving more general populations.
                - **Special education schools or schools with a high percentage of students with disabilities**:
                    - We flag schools that are explicitly identified as special education schools by the Common Core of Data (CCD) or the Civil Rights Data Collection (CRDC). We also flag schools where more than 40% of students are identified as having a disability in the CRDC data. Students with disabilities are identified per the Individuals with Disabilities Education Act (IDEA).
                - **Schools with a high percentage of students in gifted/talented programs or with selective admissions**:
                    - We flag schools where more than 40% of the students are enrolled in a gifted/talented program according to the CRDC data. We also flag some schools with selective-admission policies (schools where students must pass a test to be admitted), but we do not have a comprehensive list of such schools, so not all selective-admissions schools are yet identified in our data.
                - **Schools with a high percentage of limited English-proficient students**:
                    - We flag schools where more than 50% of the students are identified as limited English proficient (LEP) in the CRDC data. LEP students are classified by state definitions based on Title IX of the Elementary and Secondary Education Act (ESEA).
            - The downloadable SEDA data files include student, community, school, and district characteristics that researchers and others can use.
        - How is socioeconomic status measured?
            - For each school district or county, we use data from the Census Bureau’s American Community Survey (ACS) to create estimates of the average socioeconomic status (SES) of families. Every year, the ACS surveys families in each community in the U.S. We use six community characteristics reported in the surveys from 2007 through 2016 to construct a composite measure of SES in each community:
                - Median income
                - Percentage of adults age 25 and older with a bachelor’s degree or higher
                - Poverty rate among households with children age 5–17
                - Percentage of households receiving benefits from the Supplemental Nutrition Assistance Program (SNAP)
                - Percentage of households headed by single mothers
                - Employment rate for adults age 25–64.
            - The composite SES measure is standardized so that a value of 0 represents the SES of the average school district in the U.S. Approximately two-thirds of districts have SES values between -1 and +1, and approximately 95% have SES values between -2 and +2. In some places we cannot calculate a reliable measure of socioeconomic status, because the ACS samples are too small; in these cases, no value for SES is reported. For more detailed information, please see the technical documentation.
        - What does the gap in school poverty measure?
            - The gap in school poverty (shown in the secondary gap charts) is a measure of school segregation. We use the proportion of students defined as “economically disadvantaged” in a school as a measure of school poverty. The black-white gap in school poverty, 
                - for example, measures the difference between the poverty rate of the average black student’s school and the poverty rate of the average white student’s school. When there is no segregation—when white and black students attend the same schools, or when white and black students’ schools have equal poverty rates—the black-white school-poverty gap is 0. A positive black-white school-poverty gap means that black students’ schools have higher poverty rates than white students’ schools, on average. A negative black-white school-poverty gap means that white students’ schools have higher poverty rates than black students’ schools, on average.
        - What does the gap in percent minority students in schools measure? 
            - The gap in percent minority students in schools (shown in the secondary gap charts) is a measure of school segregation. Percent minority students in schools is measured as the proportion of minority students (black students plus Hispanic students) in a student’s school. The black-white gap in percent minority students in schools then measures the difference between the proportion of minority students in the average black student’s school and the proportion of minority students in the average white student’s school. 
                - When there is no segregation—when white and black students attend the same schools, or when white and black students’ schools have equal proportions of minority students—the black-white gap in percent minority students is 0.
                - A positive black-white gap in percent minority students means that black students’ schools have higher shares of minority students than white students’ schools, on average. A negative black-white gap in percent minority students means that white students’ schools have higher shares of minority students than black students’ schools, on average.
        - Why are the data here different from the results reported by my state?
            - SEDA results may differ from publicly reported state test scores.
            - States typically report test scores in terms of percentages of proficient students. They may also report data only for a single year and grade, or average the percentages of proficient students across grades. Measures based on the percentage of proficiency are generally not comparable across states, grades, or test subjects, and are often not comparable across years because of differences in the tests administered and differences in definitions of “proficiency.”
            - States often provide school grades or other school scores as an index. These may take a number of factors into account, not just test scores. This makes them very difficult to compare across states and grades and over time.
            - In contrast, the test-score measures we report here are based on more detailed information about students’ test scores. They are also adjusted to account for differences in tests and proficiency standards across states, years, and grades.
            - The measures we report here also differ slightly from other publicly available measures of school and district performance due to differences in statistical methods. We provide a comparison of SEDA and other types of measures in our research papers.
        - Why are there no data for my school or district? 
            - There are several reasons that SEDA may not contain data for a school, district, or county:
                - The school or district is too small and/or has too few grades to allow for an accurate estimate.
                - Fewer than 95% of students in the school or district participated in testing in the subject, grade, and year.
                - More than 40% of students in the school, district, or county took alternative assessments rather than the regular tests.
                - Data were not reported to the National Center for Education Statistics.
                - Students took different, incomparable tests within the same subject, grade, and year. This is something our methodology cannot adjust for.
                - We identified data errors that would result in inaccurate estimates.
            - For more details, please see our technical documentation.
        - Whom should I contact with additional questions?
            - Please contact our SEDA support team at sedasupport@stanford.edu
        - How can I learn more? 
            - The Discoveries pages provide guidance on interpreting the data, the Glossary section and Methods page provide an overview of the process used to construct these data, and the technical documentation provides full methodological details along with reference for further reading.
    - Using the explorer 
        - How can I get help using the Educational Opportunity Explorer? 
- Methods 
    - How SEDA produced estimates of student performance that are comparable across places, grades, and years
    - Background 
        - Federal law requires all states to test all students in grades 3-8 each year in math and ELA (commonly called “accountability” testing). It also requires that states make the aggregated results of those tests public.
        - All states report test results for schools and school districts to the Department of Education’s National Center for Education Statistics (NCES), which houses this data in the EDFacts database. There are two versions of this data: a public version, available on their website, and a restricted version, available by request. For security, the public data are censored. That is, data for small places or small subgroups are not reported to ensure the privacy of the students. In contrast, the restricted files contain all data, regardless of the number of students within the school/district or subgroup.
    - Challenges Working with Proficiency Data
        - While there is a substantial amount of data from every state available in EDFacts, there are four key challenges when using these data:
        1. States provide only “proficiency data”: the count of students at each of the proficiency levels (sometimes called “achievement levels” or “performance levels”). The levels represent different degrees of mastery of the subject-specific grade-level material. Levels are defined by score “thresholds,” which are set by experts in the field. Scoring above or below different thresholds determines placement in a specific proficiency level. Common levels include “below basic,” “basic,” “proficient,” and “advanced.” An example is shown below. 
            - Test Score: 200-500, 501-600, 601-700, 701-800
            - Proficiency Level: Below Basic, Basic, Proficient, Advanced
            - Description: Inadequate performance; minimal mastery, Marginal performance; partial mastery, Satisfactory performance; adequate mastery, Superior performance; complete mastery 
        2. Most states use their own test and define “proficiency” in different ways, meaning that we cannot directly compare test results in one state to those in another. Proficient in one state/grade/year/subject is not comparable to proficient in another.
            - Consider two states that use the same test, which is scored on a scale from 200 to 800 points. Each state sets its own threshold for proficiency at different scores. 
            - Example
                - Imagine 500 students take the test. The results are as follows: 50 students score below 400 on the exam; 100 score between 400 and 500; 200 score between 500 and 600; 50 score between 600 and 650; 50 score between 650 and 700; and 50 score above 700. If we use State A’s thresholds for assignment to categories, we find that 150 students are proficient. However, if we use State B’s thresholds, 350 students are proficient. 
                - In practice, this means that students in State B may appear to have higher “proficiency” rates than those in State A—even if their true achievement patterns are identical! Using the proficiency data without accounting for differing proficiency thresholds may lead to erroneous conclusions about the relative performance of students in different states.
                - This problem is more complicated than the example suggests, because most states use different tests with material of varying difficulty and scores reported on different scales. Therefore, we cannot compare proficiency, nor can we compare students’ test scores between states.
        3. Even within a state, different tests are used in different grade levels. This means that we cannot readily compare the performance of students in 4th grade in one year to that of students in 5th grade in the next year. Therefore, we cannot measure average learning rates across grades. 
        4. States may change the tests they use over time. This may result from changes in curricular standards; for example, the introduction of the Common Core State Standards led many states to adopt different tests. These changes make it hard to compare average performance in one year to that of the next. Therefore, we cannot readily measure trends in average performance over time.
    - SEDA methods: Addressing the challenges
        - While these challenges are substantial, they are not insurmountable. The SEDA team has developed methods to address these challenges in order to produce estimates of students’ average test scores, average learning rates across grades, and trends over time in each school, district, and county in the U.S. All of these estimates are comparable across states, grades, and years.
            - Below we describe the raw data used to create SEDA and how we:
                1. Estimate the location of each state's proficiency "thresholds" 
                2. Place the proficiency thresholds on the same scale
                3. Estimate the mean test scores in each school, district, and county from the raw data and the threshold estimates
                4. Scale the estimates so they are measured in terms of grade levels
                5. Create estimates of average scores, learning rates, and trends in average scores.
                6. Report the data
                7. Ensure Data Accuracy
    - Raw data
        - The data used to develop SEDA come from the EDFacts Restricted-Use Files, provided to our team via a data-use agreement with NCES.
        - EDFacts provides counts of students scoring in proficiency levels for nearly every school in the U.S. For details on missing data, see the SEDA Technical Documentation.
        - Data are available for:
            - School years 2008-09 through 2015-16
            - Grades 3-8
            - Math and English Language Arts (ELA)
            - Various subgroups of students: all students, racial/ethnic subgroups, gender subgroups, economic subgroups, etc.
        - The data also contain NCES identifiers, which allow the information to be linked to other databases or combined into different aggregations (e.g., school districts, counties, etc.).
    - Raw data description (7) 
        1. Estimating the location of each state's proficiency thresholds
            - We use a statistical technique called heteroskedastic ordered probit (HETOP) modeling to estimate the location of the thresholds that define the proficiency categories within each state, subject, grade, and year. We estimate the model using all the counts of students in each school district within a state-subject-grade-year.
            - A rough approximation of this method follows. We assume the distribution of test scores in each school district is bell-shaped. For each state, grade, year, and subject, we then find the set of test-score thresholds that meet two conditions:
                -  1) they would most closely produce the reported proportions of students in each proficiency category; and 
                - 2) they represent a test-score scale in which the average student in the state-grade-year-subject has a score of 0 and the standard deviation of scores is 1. 
            - **Example: State A, Grade 4 reading in 2014–15**
                - In the example below, there are three districts in State A. The table shows the number and proportion of scores in each of the state’s four proficiency categories. District 1 has more lower-scoring students than the others; District 3 has more higher-scoring students. We assume each district’s test-score distribution is bell-shaped. We then determine where the three thresholds would be located that would yield the proportions of students in each district shown in the table. In this example, the top threshold is one standard deviation above the statewide average score; at this value, we would expect no students from District 1 to score in the top proficiency category, and would expect 20% of those in District 3 to score in the top category.
        2. Placing the proficiency thresholds on the same scale 
        3. Estimating the mean from proficiency count data 
        4.  Scaling the Estimates to Grade Equivalents 
        5. The three parameters: Average test scores, learning rates, and trends in test scores 
        6. Reporting the data
        7. Data accuracy 
